{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DataSci/lib/python3.6/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from textrank4zh import TextRank4Keyword,TextRank4Sentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba.analyse\n",
    "import os\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET_KEYWORDS\n",
    "def get_keywords(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # TEXTRANK TOP 20\n",
    "        tr4w = TextRank4Keyword(stop_words_file='./base_data/stopword.data')\n",
    "        tr4w.train(text=x, speech_tag_filter=True, lower=True, window=2)\n",
    "        keywords_textrank = tr4w.get_keywords(10, word_min_len=2)\n",
    "        # Top_20 TF_IDF words\n",
    "        keywords_tfidf = jieba.analyse.extract_tags(x)[:10]\n",
    "        # Intersection\n",
    "        result_ = list(set(keywords_textrank).intersection(set(keywords_tfidf)))\n",
    "        return result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ_DATA\n",
    "df = pd.read_csv('./raw_data/datawithcor.csv')\n",
    "# DROP NAN\n",
    "df = df[df.Fatality_Rate.notna()]\n",
    "df = df[df.Narrative.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/pj/mrvgnnps0n92zyj8t94f613r0000gn/T/jieba.cache\n",
      "Loading model cost 0.747 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# KEYWORDS EXTRACTION\n",
    "df['key_words'] = df.Narrative.apply(lambda x: get_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET X and Y\n",
    "x_columns = ['year', 'Type', 'Operator', 'Lat', 'Long', 'key_words']\n",
    "y_columns = 'Fatality_Rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE_TRAIN\n",
    "# Lat, Long ABS\n",
    "df.Lat = df.Lat.abs()\n",
    "df.Long = df.Long.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order Map\n",
    "type_dict = {'Caproni': 0, 'Felixstowe': 1}\n",
    "operator_dict = {'Caproni': 0, 'Royal Air Force - RAF': 1}\n",
    "df.Type = df.Type.map(type_dict)\n",
    "df.Operator = df.Operator.map(operator_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF_IDF\n",
    "# 'Pre_Train, join null intro words\n",
    "df['key_words_new'] = df.key_words.apply(lambda x: ' '.join(x))\n",
    "# tfidf\n",
    "tf_train = TfidfVectorizer()\n",
    "tf_result = tf_train.fit_transform(df.key_words_new.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists('./base_data/vocabulary.json'):\n",
    "    os.remove('./base_data/vocabulary.json')\n",
    "if os.path.exists('./base_data/idfs.npy'):\n",
    "    os.remove('./base_data/idfs.npy')\n",
    "f1 = open('./base_data/vocabulary.json', 'wb')\n",
    "file = tf_train.vocabulary_\n",
    "pickle.dump(file,f1)\n",
    "f1.close()\n",
    "idfs = tf_train.idf_\n",
    "np.save('./base_data/idfs.npy',idfs)\n",
    "# \n",
    "# idfs = np.load('./base_data/idfs_con.npy')\n",
    "# x_list = [[contents_[i],add_info[i],x_list_1[i],x_list_2[i]] for i in range(len(x_list_1))]\n",
    "# vocabulary_import = pickle.load(open('./base_data/vocabulary_con.json','rb'))\n",
    "# tf_train = MyVectorizer(vocabulary = vocabulary_import)\n",
    "# tf_train._tfidf._idf_diag = sp.spdiags(idfs,\n",
    "#                                      diags = 0,\n",
    "#                                      m = len(idfs),\n",
    "#                                      n = len(idfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Merge TD_IDF'\n",
    "a = df[x_columns[:-1]]\n",
    "all_feature = np.hstack((a,  tf_result.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "# Set split\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_feature, df[y_columns],  test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  0.1362350917314941\n"
     ]
    }
   ],
   "source": [
    "# lgb\n",
    "gbm = lgb.LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=100)\n",
    "gbm.fit(x_train, y_train)\n",
    "y_pre = gbm.predict(x_test)\n",
    "print('mse: ', mean_squared_error(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=3, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:logistic', random_state=0,\n",
       "             reg_alpha=0.8, reg_lambda=0.5, scale_pos_weight=1, seed=4396,\n",
       "             silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "bat =  xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
    "             max_depth=5, min_child_weight=3, missing=None, n_estimators=100,\n",
    "             n_jobs=1, nthread=None, objective='reg:logistic', random_state=0,\n",
    "             reg_alpha=0.8, reg_lambda=0.5, scale_pos_weight=1, seed=4396,\n",
    "             silent=None, subsample=0.8,verbosity=1)\n",
    "bat.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(bat.predict(x_test),y_test)\n",
    "np.mean(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44351799921360724"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test)\n",
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
